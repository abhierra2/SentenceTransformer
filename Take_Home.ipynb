{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Sentence Transformer Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad6c052407ec433eb39907e656b874dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159c35ee558e4893ad39858f0c220c16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec7926895f5491eaa45d831a6eff046",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92ae9c072c6147b5add951930a48e573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83db86cdc9f04076b675d12ff8706063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Choice: DistilBERT was chosen for its time efficiency in inference while maintaining reasonable performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sentence Encoding Function\n",
    "def encode_sentences(sentences, tokenizer, model):\n",
    "    # Tokenize the sentences and convert to tensor inputs\n",
    "    inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Pass inputs through the transformer model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "    # Get the embeddings from the last hidden state\n",
    "    embeddings = outputs.last_hidden_state\n",
    "    \n",
    "    # CLS pooling strategy to obtain fixed-length embeddings since it is better for sentence classification\n",
    "    embeddings = embeddings[:, 0, :]\n",
    "    \n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling Strategy: CLS pooling strategy was used for time efficiency and for its single point of focus capturing high-level sentence information, making it good for sentence classification in task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 768])\n",
      "Embeddings: tensor([[-0.1665, -0.3269, -0.0461,  ..., -0.4004,  0.4681,  0.2092],\n",
      "        [ 0.0499, -0.1070, -0.1950,  ...,  0.0638,  0.3494,  0.2307],\n",
      "        [-0.1228,  0.1907, -0.2442,  ..., -0.3167,  0.2453,  0.0912]])\n"
     ]
    }
   ],
   "source": [
    "# Sample sentences\n",
    "sentences = [\n",
    "    \"The San Francisco 49ers will win the Super Bowl.\",\n",
    "    \"I bought yogurt, cheese, and honey from the grocery store.\",\n",
    "    \"Eggs are a great source for Omega fats, protein, and cholesterol.\"\n",
    "]\n",
    "\n",
    "# Encode sentences\n",
    "embeddings = encode_sentences(sentences, tokenizer, model)\n",
    "\n",
    "# Print embeddings\n",
    "print(\"Embeddings shape:\", embeddings.shape)\n",
    "print(\"Embeddings:\", embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Multi-Task Learning Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Sentence Classification Head\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense(x)\n",
    "\n",
    "# Token Classification Head for NER\n",
    "class TokenClassificationHead(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_labels):\n",
    "        super(TokenClassificationHead, self).__init__()\n",
    "        self.dense = nn.Linear(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Task model\n",
    "class MultiTaskSentenceTransformer(nn.Module):\n",
    "    def __init__(self, model_name, num_classes, num_labels):\n",
    "        super(MultiTaskSentenceTransformer, self).__init__()\n",
    "        # Load transformer backbone\n",
    "        self.transformer = AutoModel.from_pretrained(model_name)\n",
    "        \n",
    "        # Task-specific heads\n",
    "        hidden_dim = self.transformer.config.hidden_size\n",
    "        self.classification_head = ClassificationHead(hidden_dim, num_classes)\n",
    "        self.token_classification_head = TokenClassificationHead(hidden_dim, num_labels)\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, task):\n",
    "        # Pass inputs through transformer backbone\n",
    "        outputs = self.transformer(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        if task == \"classification\":\n",
    "            # Use the [CLS] token embedding for classification\n",
    "            cls_embedding = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
    "            return self.classification_head(cls_embedding)\n",
    "        \n",
    "        elif task == \"token_classification\":\n",
    "            # Use all token embeddings for token classification\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            return self.token_classification_head(token_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Output Shape: torch.Size([2, 3])\n",
      "Token Classification Output Shape: torch.Size([2, 12, 5])\n"
     ]
    }
   ],
   "source": [
    "# Define some example sentences\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Transformers are powerful models for NLP.\"\n",
    "]\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Initialize the multi-task model\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "num_classes = 3  # For sentence classification (e.g., categories like \"positive\", \"neutral\", \"negative\")\n",
    "num_labels = 5   # For NER (e.g., labels like \"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\")\n",
    "\n",
    "multi_task_model = MultiTaskSentenceTransformer(model_name, num_classes, num_labels)\n",
    "\n",
    "# Test Task A (Sentence Classification)\n",
    "classification_output = multi_task_model(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    task=\"classification\"\n",
    ")\n",
    "print(\"Classification Output Shape:\", classification_output.shape)\n",
    "\n",
    "# Test Task B (NER/Token Classification)\n",
    "token_classification_output = multi_task_model(\n",
    "    input_ids=inputs[\"input_ids\"], \n",
    "    attention_mask=inputs[\"attention_mask\"], \n",
    "    task=\"token_classification\"\n",
    ")\n",
    "print(\"Token Classification Output Shape:\", token_classification_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The transformer backbone is shared to enable efficient feature extraction across both tasks.\n",
    "- Using a classification head and a token classification head allows flexibility in handling both sentence-level and token-level predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Training Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Considerations for Freezing Different Parts of the Model\n",
    "### Freezing the Entire Network\n",
    "\n",
    "- Implications: Freezing the entire network (including the transformer backbone and task-specific heads) is typically done when using the model as a feature extractor without further training. This approach can be helpful if the pre-trained model already performs well on the new tasks or if we have limited resources for further training.\n",
    "- Advantages: Reduces computational cost and training time, as no backpropagation occurs. This is useful when only inference is needed.\n",
    "- Use if the model’s performance on the tasks is satisfactory with the current parameters and when resources or time are limited. However, this approach lacks flexibility to fine-tune the model for the specific tasks, which may result in suboptimal performance for unseen or task-specific data.\n",
    "\n",
    "### Freezing Only the Transformer Backbone\n",
    "\n",
    "- Implications: Freezing only the transformer backbone allows the task-specific heads to be trained on the new task data while preserving the general language representations learned by the transformer.\n",
    "- Advantages: Speeds up training by reducing the number of parameters to update while still adapting to new task requirements. It leverages the transformer’s general understanding of language while fine-tuning the task-specific heads to better align with each task's specific objectives.\n",
    "- This approach is ideal when the pre-trained backbone provides good base representations and only minor adjustments are needed for specific tasks. It’s particularly beneficial when fine-tuning on smaller datasets, where training the entire network might risk overfitting.\n",
    "\n",
    "### Freezing Only One Task-Specific Head\n",
    "\n",
    "- Implications: Freezing one of the task-specific heads (e.g., the sentence classification head) allows the model to adapt the other task head (e.g., NER) without altering the already well-optimized head.\n",
    "- Advantages: Allows specialization for one task while maintaining the learned features for the other. For instance, if Task A’s head (classification) has already been well-trained, freezing it can prevent task interference and allow more focused training on Task B (NER).\n",
    "- This approach is useful when one task is similar to the pre-training tasks or is already well-optimized. It’s a good choice for multi-task settings where one task requires further adaptation while the other does not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Approach\n",
    "In transfer learning, we start with a pre-trained model that has already learned general language patterns from a large corpus.\n",
    "\n",
    "### Choice of Pre-trained Model: \n",
    "- Selecting a robust model like BERT or RoBERTa is generally beneficial. These models have strong general language representations, which can transfer well to various NLP tasks.\n",
    "\n",
    "### Freezing Strategy:\n",
    "\n",
    "- Freeze lower layers of the transformer backbone, as these layers capture general language structures that are valuable across tasks.\n",
    "- Unfreeze higher layers to allow task-specific adaptations. Higher layers capture more nuanced and task-specific information, making them ideal for fine-tuning based on the new tasks’ requirements.\n",
    "- Freezing lower layers reduces the risk of overfitting, especially if training data is limited, while fine-tuning higher layers and task heads allows the model to learn task-specific nuances. In multi-task settings, this helps balance general language knowledge with specialized representations for each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Layer-wise Learning Rate Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "def get_layerwise_optimizer(model, base_lr=2e-5, layerwise_decay=0.8):\n",
    "    # Define parameter groups with different learning rates\n",
    "    optimizer_grouped_parameters = []\n",
    "    # Get all layer names in the transformer (e.g., ['layer.0', 'layer.1', ...])\n",
    "    layers = [f\"transformer.layer.{i}\" for i in range(model.transformer.config.num_hidden_layers)]\n",
    "    \n",
    "    # Divide layers into three groups: lower, middle, upper\n",
    "    # Adjust as needed for different transformer models\n",
    "    lower_layers = layers[:len(layers) // 3]\n",
    "    middle_layers = layers[len(layers) // 3: 2 * len(layers) // 3]\n",
    "    upper_layers = layers[2 * len(layers) // 3:]\n",
    "    \n",
    "    # Apply learning rates with decay for each layer group\n",
    "    lr = base_lr\n",
    "    for layer_group in [lower_layers, middle_layers, upper_layers]:\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [param for name, param in model.named_parameters() if any(layer in name for layer in layer_group)],\n",
    "                \"lr\": lr\n",
    "            }\n",
    "        ]\n",
    "        # Apply decay for the next layer group\n",
    "        lr *= layerwise_decay\n",
    "    \n",
    "    # Task-specific heads have their own learning rate, set to base_lr\n",
    "    optimizer_grouped_parameters += [\n",
    "        {\"params\": model.classification_head.parameters(), \"lr\": base_lr},\n",
    "        {\"params\": model.token_classification_head.parameters(), \"lr\": base_lr}\n",
    "    ]\n",
    "    \n",
    "    # Define optimizer with layer-wise learning rates\n",
    "    optimizer = AdamW(optimizer_grouped_parameters)\n",
    "    return optimizer\n",
    "\n",
    "# Example usage\n",
    "model = MultiTaskSentenceTransformer(model_name=\"distilbert-base-uncased\", num_classes=3, num_labels=5)\n",
    "optimizer = get_layerwise_optimizer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "- We divide the transformer layers into lower, middle, and upper groups to apply different learning rates, which gradually decrease as we go deeper into the model.\n",
    "- layerwise_decay controls the rate at which learning rates decrease for each layer group. A value of 0.8 means each subsequent group has an 80% learning rate of the previous group.\n",
    "- Task-specific heads receive the base learning rate, as they typically require more fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benefits of Layer-Wise Learning Rates\n",
    "- Lower layers preserve general language knowledge by learning at a slower rate, while higher layers adapt more readily to new tasks.\n",
    "- Different tasks may benefit from more adjustments at different layers. Layer-wise learning rates allow each task to benefit from the shared representation without overfitting or underfitting lower layers.\n",
    "- By focusing higher learning rates on task-relevant layers, we reduce unnecessary adjustments to general language patterns learned by the lower layers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
